---
title: "126 Final Project Step 3"
author: "Angel Abdulnour, Andrew Hansen, Sammy Suliman"
date: "2023-05-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(generics)
library(skimr)
library(dplyr)
library(caret)
library(tidyverse)
library(ggplot2)
#install.packages("olsrr")
#install.packages("lmtest")
#install.packages("GGally")
#install.packages("MASS")
library("lmtest")
library("olsrr")
library(GGally)
library(MASS)
library(recipes)
library(Metrics)
```

```{r, echo=FALSE}
#For Sammy to read the file in
#performance <- read.csv("C:/Users/filto/Desktop/PSTAT_126/project2/student/student-por.csv", header=TRUE, stringsAsFactors=FALSE, sep=';')

#For Angel to read the file in
performance <- read.csv("C:/Users/Angel/OneDrive/Desktop/schoolwork/1. Pstat 126/pstat-126/student/student-por.csv", header=TRUE, stringsAsFactors=FALSE, sep=';')

#performance

#skim(performance)
```

Review: The UCI student performance dataset, collected from students at a secondary school in Portugal, has 16 numeric variables. Of those we have the three response variables, G1, G2, and G3, which represent the student's grades in each quarter respectively (for the sake of our project we will be using G3). There are 13 numeric predictor variables, and 17 categorical predictors.

First, we will partition the data into a training and testing set for future validation purposes.

```{r}
#make this example reproducible
set.seed(1)

#create ID column
performance$id <- 1:nrow(performance)

#use 70% of dataset as training set and 30% as test set 
train <- performance %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(performance, train, by = 'id')
```

```{r, warning=FALSE, echo=FALSE}
ggpairs(performance, columns = c('Medu', 'Fedu', 'Walc', 'Dalc', 'goout'))
```

We see from our pairsplot of a limited selection of numeric potential predictors, that the pairs of variables 'Dalc'/'Walc' (weekday / weekend consumption of alcohol) and 'Medu/Fedu' (mothers'/fathers' education level) are understandably highly correlated. So we will only consider one of these pairs as potential predictors in our model.

```{r}
numeric_performance <- performance %>% select_if(is.numeric)
summary(lm(G3 ~ ., data=numeric_performance))
```
We see that Dalc appears to be a more significant predictor than Walc based on p-values, so we will drop Walc and Medu from our dataset. 

We will now perform one-hot encoding on all of the categorical variables as part of feature engineering.

```{r, echo=FALSE, warning=FALSE}
train2 <- train %>% dplyr::select(-G1, -G2, -Medu, -Walc)
test2 <- test %>% dplyr::select(-G1, -G2, -Medu, -Walc)
train3 <- train2
train3$school <- factor(train3$school, exclude = NULL)
train3$sex <- factor(train3$sex, exclude = NULL)
train3$address <- factor(train3$address, exclude = NULL)
train3$famsize <- factor(train3$famsize, exclude = NULL)
train3$Pstatus <- factor(train3$Pstatus, exclude = NULL)
train3$Mjob <- factor(train3$Mjob, exclude = NULL)
train3$Fjob <- factor(train3$Fjob, exclude = NULL)
train3$reason <- factor(train3$reason, exclude = NULL)
train3$paid <- factor(train3$paid, exclude = NULL)
train3$guardian <- factor(train3$guardian, exclude = NULL)
train3$schoolsup <- factor(train3$schoolsup, exclude = NULL)
train3$famsup <- factor(train3$famsup, exclude = NULL)
train3$activities <- factor(train3$activities, exclude = NULL)
train3$nursery <- factor(train3$nursery, exclude = NULL)
train3$higher <- factor(train3$higher, exclude = NULL)
train3$internet <- factor(train3$internet, exclude = NULL)
train3$romantic <- factor(train3$romantic, exclude = NULL)
train3 <- model.matrix(~.-1, data=train3[c('school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'paid', 'guardian', 'schoolsup', 'famsup', 'activities', 'nursery', 'higher', 'internet', 'romantic')],
                       contrasts.arg = list(school = contrasts(train3$school, contrasts = FALSE),
                                            sex = contrasts(train3$sex, contrasts = FALSE),
                                            address = contrasts(train3$address, contrasts = FALSE),
                                            famsize = contrasts(train3$famsize, contrasts = FALSE),
                                            Pstatus = contrasts(train3$Pstatus, contrasts = FALSE),
                                            Mjob = contrasts(train3$Mjob, contrasts = FALSE),
                                            Fjob = contrasts(train3$Fjob, contrasts = FALSE),
                                            reason = contrasts(train3$reason, contrasts = FALSE),
                                            paid = contrasts(train3$paid, contrasts = FALSE),
                                            guardian = contrasts(train3$guardian, contrasts = FALSE),
                                            schoolsup = contrasts(train3$schoolsup, contrasts = FALSE),
                                            famsup = contrasts(train3$famsup, contrasts = FALSE),
                                            activities = contrasts(train3$activities, contrasts = FALSE),
                                            nursery = contrasts(train3$nursery, contrasts = FALSE),
                                            higher = contrasts(train3$higher, contrasts = FALSE),
                                            internet = contrasts(train3$internet, contrasts = FALSE),
                                            romantic = contrasts(train3$romantic, contrasts = FALSE)))
train3 <- data.frame(train3)
train3 <- train3 %>% dplyr::select(-schoolMS, -sexF, -addressU, -famsizeLE3, -PstatusT, -Mjobteacher, -Fjobteacher, -reasonhome, -guardianmother, -schoolsupyes, -famsupyes, -paidyes, -activitiesyes, -nurseryyes, -higheryes, -internetyes, -romanticyes)

train3['G3'] <- train2['G3']
numeric_performance <- train2 %>% select_if(is.numeric)
df_list <- list(numeric_performance, train3)
train3 <- df_list %>% reduce(full_join, by='G3')
train3 <- train3 %>% dplyr::select(-id)
```

```{r, echo=FALSE, warning=FALSE, verbose=TRUE}
test3 <- test2
test3$school <- factor(test3$school, exclude = NULL)
test3$sex <- factor(test3$sex, exclude = NULL)
test3$address <- factor(test3$address, exclude = NULL)
test3$famsize <- factor(test3$famsize, exclude = NULL)
test3$Pstatus <- factor(test3$Pstatus, exclude = NULL)
test3$Mjob <- factor(test3$Mjob, exclude = NULL)
test3$Fjob <- factor(test3$Fjob, exclude = NULL)
test3$reason <- factor(test3$reason, exclude = NULL)
test3$paid <- factor(test3$paid, exclude = NULL)
test3$guardian <- factor(test3$guardian, exclude = NULL)
test3$schoolsup <- factor(test3$schoolsup, exclude = NULL)
test3$famsup <- factor(test3$famsup, exclude = NULL)
test3$activities <- factor(test3$activities, exclude = NULL)
test3$nursery <- factor(test3$nursery, exclude = NULL)
test3$higher <- factor(test3$higher, exclude = NULL)
test3$internet <- factor(test3$internet, exclude = NULL)
test3$romantic <- factor(test3$romantic, exclude = NULL)
test3 <- model.matrix(~.-1, data=test3[c('school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'paid', 'guardian', 'schoolsup', 'famsup', 'activities', 'nursery', 'higher', 'internet', 'romantic')],
                       contrasts.arg = list(school = contrasts(test3$school, contrasts = FALSE),
                                            sex = contrasts(test3$sex, contrasts = FALSE),
                                            address = contrasts(test3$address, contrasts = FALSE),
                                            famsize = contrasts(test3$famsize, contrasts = FALSE),
                                            Pstatus = contrasts(test3$Pstatus, contrasts = FALSE),
                                            Mjob = contrasts(test3$Mjob, contrasts = FALSE),
                                            Fjob = contrasts(test3$Fjob, contrasts = FALSE),
                                            reason = contrasts(test3$reason, contrasts = FALSE),
                                            paid = contrasts(test3$paid, contrasts = FALSE),
                                            guardian = contrasts(test3$guardian, contrasts = FALSE),
                                            schoolsup = contrasts(test3$schoolsup, contrasts = FALSE),
                                            famsup = contrasts(test3$famsup, contrasts = FALSE),
                                            activities = contrasts(test3$activities, contrasts = FALSE),
                                            nursery = contrasts(test3$nursery, contrasts = FALSE),
                                            higher = contrasts(test3$higher, contrasts = FALSE),
                                            internet = contrasts(test3$internet, contrasts = FALSE),
                                            romantic = contrasts(test3$romantic, contrasts = FALSE)))
test3 <- data.frame(test3)
test3 <- test3 %>% dplyr::select(-schoolMS, -sexF, -addressU, -famsizeLE3, -PstatusT, -Mjobteacher, -Fjobteacher, -reasonhome, -guardianmother, -schoolsupyes, -famsupyes, -paidyes, -activitiesyes, -nurseryyes, -higheryes, -internetyes, -romanticyes)

test3['G3'] <- test2['G3']
numeric_performance <- test2 %>% select_if(is.numeric)
df_list <- list(numeric_performance, test3)
test3 <- df_list %>% reduce(full_join, by='G3')
test3 <- test3 %>% dplyr::select(-id)
```


```{r, warning=FALSE, echo=FALSE}
ggpairs(train3, columns = c('Mjobat_home', 'Fjobat_home', 'Mjobhealth', 'Fjobhealth'))
```
Correlation between Mjob / Fjob factors is not as strong as we expected - we don't have to drop those columns 
We can now move on to fitting a model with all of our predictor variables.

```{r}
model3 <- lm(G3 ~ . + romanticno:studytime + activitiesno:studytime + higherno:absences, data=train3)
summary(model3)
```

## Variable Selection

Now that we have fitted the full model with all of our predictors and 3 interaction terms between categorical and numeric variables we though might have a different association with the response at different levels, we want to try backwards selection to reduce the model so it only has statistically significant predictors. We will use AIC (Akaike information criterion) as our metric to evaluate this to maximize predictive power.

```{r, warning=FALSE}
backward <- stepAIC(model3, direction='backward', trace=0)
summary(backward)
```
The only variables that were dropped by the backward selection algorithm was reasonreputation, romanticno:studytime, and activitiesno:studytime. Their large p-value in the full model indicated that they were not statistically significant predictor. 

## Interaction effects

We have decided to use interaction terms for the following reasons: Firstly, it is clear that the numeric variables will have a different association with the response at different levels of the categorical factors, for example, why the same amount of absences would have a different association with the response based on whether or not the subject wanted to pursue higher education, etc. Seeing as how the backward selection algorithm has preserved this term, we believe this validates our intuition as to the significance of the interaction.

## Computation Model

```{r, warning=FALSE}
new_model <- lm(G3 ~ studytime + age + romanticno, data=train3)
summary(new_model)
```

```{r, warning=FALSE}
new_model2 <- lm(G3 ~ studytime + age + romanticno + age:romanticno, data=train3)
summary(new_model2)
```
The 2 models we will be using are a multiple linear regression model, 'new_model' containing 2 numeric predictors, age and studytime, as well as 1 categorical predictor 'romanticno', and a second model 'new_model2', containing the same predictors as new_model, but with an additional interaction term between romanticno and age. We chose these 2 new models to test whether our assertion that the interaction terms were not needed was justified.


```{r, warning=FALSE}
predictions <- predict(new_model, test3)

data.frame(
  R2 = R2(predictions, test3$G3),
  RMSE = rmse(predictions, test3$G3),
  MAE = mae(predictions, test3$G3)
)
```

```{r, warning=FALSE}
predictions2 <- predict(new_model2, test3)

data.frame(
  R2 = R2(predictions2, test3$G3),
  RMSE = rmse(predictions2, test3$G3),
  MAE = mae(predictions2, test3$G3)
)
```

To evaluate the performance of our 2 models, we chose the metrics $R^2$, RMSE (Root Mean Squared Error), and MAE (Mean Absolute Error). $R^2$ is a measure of goodness of fit, whereas RMSE and MAE are measures of total error. After validating our models on the testing set, in all 3 metrics, the extended model including the interaction term performed nearly identically to the model without the interaction term. This indicates that including an interaction term between the numeric and categorical variables in our dataset may not have a large effect on model fit in some cases.

## Statistical model

We already computed the model 'backward', through the backward selection algorithm, starting with the full model involving all predictors in the training data, and testing p-values to drop the least significant predictors (in this case, only 1). To justify our model, we will plot a graph of residuals against fit to check whether the core assumptions of the linear model (linearity and constant variance) are satisfied.

```{r}
res <- resid(backward)
plot(fitted(backward), res)
```

In the visual above we can see that variance is not constant so we try this procedure again, this time we transform the response variable by taking the square root of G3.

```{r, echo=TRUE, results='hide'}
model4 <- lm(sqrt(G3) ~ . + romanticno:studytime + activitiesno:studytime + higherno:absences, data=(train3))
summary(model4)
backward2 <- stepAIC(model4, direction='backward', trace=0)
summary(backward2)
```

This time more predictors are dropped by the backward selection algorithm, goout, famsizeGT3, PstatusA, in addition to reasonreputation.

```{r, echo=FALSE}
res2 <- resid(backward2)
plot(fitted(backward2), res2)
```
However, the residuals vs fit plot still suggests the linearity and variance assumptions are violated.

```{r}
train4 <- train3
train4$G3[train4$G3 == 0] <- 1e-50
model5 <- lm(log(G3) ~ ., data=train4)
backward3 <- stepAIC(model5, direction='backward', trace=0)
```

```{r}
res3 <- resid(backward3)
plot(fitted(backward3), res3)
```
A log transformation on the response also yields an unsatisfactory result. Let's plot the residuals vs fit graph for our previously fitted smaller-sized model. 

```{r, echo=FALSE}
res2 <- resid(new_model2)
plot(fitted(new_model2), res2)
```
With the exception of some outlying points which we will examine later, we see the assumptions of the linear model appear to be satisfied. So it seems that perhaps the issue with the 'backward' model is too many predictors. Our smaller model only contains the most significant predictors of our model (as judged by the p-values of these variables in our 'backward' summary), plus an interaction term that intuitively feels reasonable (being in a romantic relationship may affect performance even after controlling for age).

Since the residuals vs fit plot appears to satisfy the most important assumptions of the linear model, we will adopt this model going forward.

```{r}
overall_p <- function(model) {
    f <- summary(model)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}

print(paste("P-value:", overall_p(new_model2)))
```

The very low p-value confirms the significance of regression for our model.

## Significance of Coefficients

```{r, echo=FALSE}
# Obtain the coefficient p-values
coef_p_values <- summary(new_model2)$coefficients[, "Pr(>|t|)"]

# Define the desired significance level (e.g., 0.05)
alpha <- 0.05

# Apply Bonferroni correction
adjusted_alpha <- alpha / length(train3$G3)

# Print the results
for (i in 1:length(coef_p_values)) {
  # Calculate the t-value and p-value
  t_result <- t.test(train3$G3, alternative = "two.sided", mu = 0)
  
  if (t_result[i]$p.value < adjusted_alpha) {
    print(paste("Coefficient", names(coef_p_values)[i], "is significant."))
  } else {
    print(paste("Coefficient", names(coef_p_values)[i], "is not significant."))
  }
}

```


```{r}
# Obtain the coefficients
coefficients <- coef(new_model2)

# Define the desired significance level (e.g., 0.05)
alpha <- 0.05

# Apply Bonferroni correction
adjusted_alpha <- alpha / length(train3$G3)

# Perform the t-test for each coefficient
for (i in 1:(length(coefficients)-1)) {
  # Calculate the t-value and p-value
  beta_hat_i <- coef(new_model2)[names(coefficients)[i]]
  sigmasqhat <- summary(new_model2)$sigma^2
  x_mx <- model.matrix(new_model2)
  xtx_inv <- solve(t(x_mx) %*% x_mx)
  se_beta_hat_i <- sqrt(sigmasqhat * xtx_inv[i, i])
  test_stat <- beta_hat_i / se_beta_hat_i
  pval <- 2*(1-pt(abs(test_stat), df = new_model2$df.residual))
  # Check if the p-value is less than the significance level
  if (pval < adjusted_alpha) {
    print(paste("Coefficient", names(coefficients)[i], "is significant."))
  } else {
    print(paste("Coefficient", names(coefficients)[i], "is not significant."))
  }
}

beta_hat_5 <- coef(new_model2)['age:romanticno']
sigmasqhat <- summary(new_model2)$sigma^2
x_mx <- model.matrix(new_model2)
xtx_inv <- solve(t(x_mx) %*% x_mx)
se_beta_hat_5 <- sqrt(sigmasqhat * xtx_inv[5, 5])
test_stat <- beta_hat_5 / se_beta_hat_5
pval <- 2*(1-pt(abs(test_stat), df = new_model2$df.residual))
if (pval < adjusted_alpha){
  print("Coefficient age:romanticno is significant.")
  } else {
    print("Coefficient age:romanticno is not significant.")
}
```



Even after applying the Bonferroni correction to control for the family-wise error rate, we see that only age and studytime remain significant.

```{r}
new_model2 <- lm(G3 ~ age + studytime, data=train3)
```


## Goodness of Fit

```{r, echo=FALSE}
predictions3 <- predict(new_model2, data=test3)
R2 <- R2(predictions, test3$G3)

adj_r2 <- function(x) {
   return (1 - ((1-R2)*(nobs(x)-1)/(nobs(x)-length(x$coefficients)-1)))
}

data.frame(
  R2 = R2(predictions2, test3$G3),
  Adj_R2 = adj_r2(new_model2)
)

```
Both the unadjusted and adjusted $R^2$ values are rather low. This means that relatively little of the variance of the data can be explained by our model. However, $R^2$ is not necessarily the best metric for model fit as it increases with number of predictors, regardless of whether they are significant.

## Influence

```{r, echo=FALSE}
a <- seq(1, 65928)
p_caseinf <- broom::augment(new_model2, train3) %>%
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  ggplot(aes(x = a, y = value)) +
  facet_wrap(~ name, scales = 'free_y', nrow = 3) + # looks better with vertical faceting
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + # rotates and aligns labels
  labs(x = '', y = '')

p_caseinf
```
Let's identify the biggest outliers, leverage points, and points of greatest influence.

```{r}
# most influential points
high_influence <- augment(new_model2, train3) %>%
  slice_max(order_by = .cooksd, n = 5) %>%
  mutate(row_index = row_number())
high_influence
```

```{r}
# most outlying points
high_residuals <- augment(new_model2, train3) %>%
  slice_max(order_by = .resid, n = 5) %>%
  mutate(row_index = row_number()) 
high_residuals
```

```{r}
# highest leverage points
high_leverage <- augment(new_model2, train3) %>%
  slice_max(order_by = .hat, n = 5) %>%
  mutate(row_index = row_number()) 
high_leverage
```

```{r}
b <- seq(1, 80)
unusual_obs <- broom::augment(new_model2, train3) %>% 
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  group_by(name) %>%
  slice_max(order_by = abs(value), n = 3) %>%
  ungroup()
p_caseinf + geom_point(data = unusual_obs, color = 'red', aes(x=b, y=value))
```

Now lets try to exclude observations with high leverage and high residuals from the training set and refit our model.
```{r}
high_leverage['.resid']
```

```{r}
(high_residuals['.hat'])
```
We notice there is a single 'high leverage' observation with higher than average residuals. We will test the effect of retraining our model without this observation.
```{r, echo=FALSE}
high_resid_high_leverage <- high_leverage[1,]
print(paste("Cook's distance:", high_resid_high_leverage$.cooksd))
```
This observation has a relatively large Cook's distance, which indicates it is relatively influential to the model fit.

```{r, echo=FALSE}
# we see absences = 12, age = 22 ... use this to find index of observation in original training set
train3[train3$absences == 12 & train3$age == 22,]
# row index = 1596
train4 <- train3[-c(1596),]
```

```{r}
new_model3 <- lm(G3 ~ studytime + age, data=train4)
summary(new_model3)
```
It appears that none of our coefficients have changed since we dropped the high leverage / high residual observation from our training set.

## Confidence / Prediction Intervals for $\hat{Y}$

Next, we will examine a confidence interval for the mean of the response (G3) at the mean value of the numeric predictor variables and at the most common value of the categorical predictor values:
```{r, echo=FALSE}
freq_value <- function(x) {
as.numeric(tail(names(sort(table(x))), 1))
}
```

```{r, echo=FALSE}
x_bar <- c(freq_value(test3$studytime), mean(test3$age))
new_data <- data.frame(studytime = x_bar[1], age = x_bar[2])
```

```{r, echo=FALSE}
predict(new_model2, newdata = new_data, interval = 'confidence', level = 0.95)
```
With 95% confidence, the final grade for a student with age equal to the mean and with the most common relationship status (single), and in the most common studytime bracket (2-5 hours) is between 11.78082 and 11.83773.

```{r, echo=FALSE}
new_data2 <- data.frame(age = 22, studytime = 4)
predict(new_model2, newdata = new_data2, interval = 'prediction', level = 0.95)
```
With 95% confidence, the final grade for a student aged 22, in a relationship, and who studies between 5-10 hours a week is between 8.245044 and 16.56138.

## Summary

We began our analysis by dropping 2 variables, 'Dalc' and 'Medu', that exhibited a high degree of correlation with the other variables in our dataset. We then encoded all $n$ levels of each categorical variable as $n-1$ separate binary variables. We then used the backward selection algorithm with the Alkaike Information criterion for variable selection. However, the result of this model failed to satisfy the conditions for linear regression, as seen in diagnostic plots. So we instead chose a simpler model with a single interaction term. We confirmed the significance of 2 of the predictors, however, we received low $R^2$ and adjusted $R^2$ and experimented with the effect of dropping a single highly influential observation only to find it failed to affect the significance. 
